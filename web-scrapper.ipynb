{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapper Automatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resquests website\n",
    "section_url='https://www.ultimahora.com/contenidos/nacional.html'\n",
    "resquests_section_url= requests.get(section_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verify the website is online\n",
    "resquests_section_url.status_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts the elements of the news\n",
    "\n",
    "def get_info(url):\n",
    "    dict={}\n",
    "    r_notice= requests.get(url)\n",
    "    s_notice= BeautifulSoup(r_notice.text, 'lxml')\n",
    "\n",
    "    #title notice\n",
    "    title=s_notice.find('h1', attrs={'class':'title'}).text\n",
    "    if title:\n",
    "        dict['title']=title\n",
    "    else:\n",
    "        dict['title']=None\n",
    "\n",
    "    #date notice\n",
    "    time=s_notice.find('time', attrs={'class':'date'}).text\n",
    "    if time:\n",
    "        dict['time']=time\n",
    "    else:\n",
    "        dict['time']=None\n",
    "\n",
    "    #lead\n",
    "    lead=s_notice.find('h2', attrs={'class':'excerpt'}).text\n",
    "    if time:\n",
    "        dict['lead']=lead\n",
    "    else:\n",
    "        dict['lead']=None\n",
    "    \n",
    "    #body\n",
    "    b=''\n",
    "    body=s_notice.find('div', attrs={'class':'body-content note-body'}).find_all('p')\n",
    "    for p in body:\n",
    "        b=b+''+(p.text)\n",
    "    if b:\n",
    "        dict['body']=b\n",
    "    else:\n",
    "        dict['body']=None\n",
    "\n",
    "    #img\n",
    "    media = s_notice.find('div', attrs={'class':'image'}).find('img')\n",
    "    img_src=media.get('data-td-src-property')\n",
    "    try:\n",
    "        r_img=requests.get(img_src)\n",
    "        if r_img.status_code==200:\n",
    "            dict['img']=r_img.content\n",
    "        else:\n",
    "            dict['img']=None\n",
    "    except:\n",
    "        print('there is not image')\n",
    "\n",
    "    return dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_info('https://www.ultimahora.com/corte-rechaza-accion-contra-ordenanza-que-limita-distancia-gasolineras-asuncion-n3040554.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_url(url):\n",
    "    try:\n",
    "        r=requests.get(url)\n",
    "    except Exception as e:\n",
    "        print('Error ',url)\n",
    "        print (e)\n",
    "        return None\n",
    "    \n",
    "    if r.status_code !=200:\n",
    "        print(f'Error status code {url}')\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    dict=get_info(url)\n",
    "    dict['url']=url\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper_url('https://www.ultimahora.com/corte-rechaza-accion-contra-ordenanza-que-limita-distancia-gasolineras-asuncion-n3040554.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_page_url(url):\n",
    "    list_next_page_url=[url]\n",
    "\n",
    "    for i in range(5):\n",
    "        r= requests.get(list_next_page_url[-1])\n",
    "        s= BeautifulSoup(r.text, 'lxml')\n",
    "        get_next_page= s.find('a', attrs={'class':'right-button'})\n",
    "        get_url=get_next_page.get('href')\n",
    "        list_next_page_url.append(get_url)\n",
    "    \n",
    "    return list_next_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next_page_url('https://www.ultimahora.com/contenidos/nacional.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the links of the notices\n",
    "def get_notice_url(url):\n",
    "    all_notice_url=[]\n",
    "    for i in url:\n",
    "        try:\n",
    "            r=requests.get(i)\n",
    "            s=BeautifulSoup(r.text, 'lxml')\n",
    "            xpath_section_url= etree.HTML(str(s))\n",
    "            links_notices=xpath_section_url.xpath(\"//div[@class='col-12 col-lg-8']//*[@class='article-title']//a\")\n",
    "            \n",
    "            #Save links in a list\n",
    "            for i in links_notices:\n",
    "                link=i.get(\"href\")\n",
    "                all_notice_url.append(link)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print ('Error \\n')\n",
    "            print(e)\n",
    "    \n",
    "    return all_notice_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_notice_url(get_next_page_url('https://www.ultimahora.com/contenidos/nacional.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapper\n",
    "data=[]\n",
    "all_URLs=get_notice_url(get_next_page_url(section_url))\n",
    "for contador, url in enumerate(all_URLs):\n",
    "    print (f'scraper link: {contador}/{len(all_URLs)}')\n",
    "    data.append(scraper_url(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an excel document with the obtained data\n",
    "df.to_excel('scraper.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97e6cad3ecb9bcade12322525e6936341a6cce36f71e8d97aba20d12930216bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
